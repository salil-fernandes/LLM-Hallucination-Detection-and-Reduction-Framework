{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40add2e1-2976-48f0-90ea-6c34aa05ac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datasets import Dataset\n",
    "import wikipedia\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from groq import Groq\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5a1d39-7fc8-433b-a359-486d7bc9b4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'right_answer', 'hallucinated_answer',\n",
       "       'together_llama_answer', 'together_llama_is_hallucinated',\n",
       "       'together_gemma_answer', 'together_gemma_is_hallucinated',\n",
       "       'groq_qwen_answer', 'groq_qwen_is_hallucinated'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hallucination_output.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83583458-5f55-4439-9001-59c0158e6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_llama = df.dropna(subset=[\"question\", \"right_answer\", \"together_llama_answer\"]).copy()\n",
    "df_valid_gemma = df.dropna(subset=[\"question\", \"right_answer\", \"together_gemma_answer\"]).copy()\n",
    "df_valid_qwen = df.dropna(subset=[\"question\", \"right_answer\", \"groq_qwen_answer\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020ac792-56c7-4730-998d-62c0144561ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load NLI pipelines\n",
    "nli_pipe = pipeline(\"text-classification\", model=\"roberta-large-mnli\", device=0)\n",
    "fact_checker = pipeline(\"text-classification\", model=\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\", device=0)\n",
    "\n",
    "# Define vague phrases\n",
    "vague_phrases = [\n",
    "    \"i don't know\", \"i do not know\", \"i'm not sure\", \"unable to\", \"no information\",\n",
    "    \"not enough information\", \"not available\", \"unfortunately\", \"i cannot verify\",\n",
    "    \"cannot be confirmed\", \"data is missing\", \"unknown\", \"n/a\", \"none found\",\n",
    "    \"there is no\", \"i do not have access\"\n",
    "]\n",
    "\n",
    "def is_vague(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text = text.lower()\n",
    "    return any(phrase in text for phrase in vague_phrases)\n",
    "\n",
    "def clean_markdown(text):\n",
    "    return re.sub(r'[*_~`]+', '', text)\n",
    "\n",
    "def get_first_sentence(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return sentences[0] if sentences else text\n",
    "\n",
    "def verify_claim_with_wikipedia(claim):\n",
    "    try:\n",
    "        search_results = wikipedia.search(claim)\n",
    "        if not search_results:\n",
    "            return \"NO WIKI RESULTS\", None\n",
    "        page = wikipedia.page(search_results[0])\n",
    "        premise = page.summary\n",
    "        nli_input = f\"{premise} [SEP] {claim}\"\n",
    "        result = fact_checker(nli_input, truncation=True)[0]\n",
    "        return result['label'], result['score']\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", str(e)\n",
    "\n",
    "def run_nli_with_vagueness_and_factual_check(data, answer_column, batch_size=16, name='llama'):\n",
    "    def build_input(row):\n",
    "        evidence = f\"The answer to the question is '{row['right_answer']}'.\"\n",
    "        claim = row[answer_column]\n",
    "        return f\"{evidence} [SEP] {claim}\"\n",
    "\n",
    "    data[\"nli_input\"] = data.apply(build_input, axis=1)\n",
    "    hf_dataset = Dataset.from_pandas(data)\n",
    "\n",
    "    def run_nli_batch(batch):\n",
    "        results = nli_pipe(batch[\"nli_input\"], truncation=True)\n",
    "        return {\n",
    "            \"nli_label\": [r[\"label\"] for r in results],\n",
    "            \"nli_score\": [r[\"score\"] for r in results]\n",
    "        }\n",
    "\n",
    "    hf_dataset = hf_dataset.map(run_nli_batch, batched=True, batch_size=batch_size, desc=f\"NLI for {name}\")\n",
    "\n",
    "    def factuality_callback(row):\n",
    "        label = row[\"nli_label\"]\n",
    "        score = row[\"nli_score\"]\n",
    "        claim_text = row[\"claim_text\"]\n",
    "        vague = is_vague(claim_text)\n",
    "    \n",
    "        if label.lower() == \"neutral\":\n",
    "            first_sentence = get_first_sentence(clean_markdown(claim_text))\n",
    "            wiki_label, wiki_score = verify_claim_with_wikipedia(first_sentence)\n",
    "            label = f\"NEUTRAL → {wiki_label}\"\n",
    "            score = wiki_score if isinstance(wiki_score, float) else 0.0\n",
    "    \n",
    "        return {\"label\": label, \"score\": score, \"is_vague\": vague}\n",
    "\n",
    "    # Parallel processing\n",
    "    rows = [{\n",
    "        \"nli_label\": hf_dataset[i][\"nli_label\"],\n",
    "        \"nli_score\": hf_dataset[i][\"nli_score\"],\n",
    "        \"claim_text\": row[answer_column]\n",
    "    } for i, row in data.iterrows()]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(tqdm(executor.map(factuality_callback, rows), total=len(rows)))\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f\"./nli_predictions/nli_{name}_results.csv\", index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e21329-f7ed-446a-997f-7f330a4d6d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcb0b8e212a4ec8b30ec2bde8a1eee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NLI for gemma:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  0%|          | 18/10000 [00:01<14:50, 11.21it/s] C:\\Users\\salil\\anaconda3\\envs\\NLP\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\salil\\anaconda3\\envs\\NLP\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "100%|██████████| 10000/10000 [11:58<00:00, 13.92it/s] \n"
     ]
    }
   ],
   "source": [
    "gemma_nli_results = run_nli_with_vagueness_and_factual_check(df_valid_gemma.copy(), \"together_gemma_answer\", name=\"gemma\").to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd42908-8a9b-4800-8514-9fffee859231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nli(results, human_verified, threshold=0.65):\n",
    "    def nli_to_pred(entry):\n",
    "        if entry.get(\"is_vague\", False):  # override for vague responses\n",
    "            return True\n",
    "        return (\"contradiction\" in entry[\"label\"].lower()) and entry[\"score\"] >= threshold\n",
    "\n",
    "    predicted_labels = [nli_to_pred(r) for r in results]\n",
    "    true_labels = human_verified.tolist()\n",
    "\n",
    "    false_negatives = [\n",
    "        i for i, (true, pred) in enumerate(zip(true_labels, predicted_labels))\n",
    "        if true and not pred\n",
    "    ]\n",
    "\n",
    "    return classification_report(true_labels, predicted_labels), false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ddecfa0-9622-4244-ac67-6e13c60b2e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma NLI Metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.91      0.69      5268\n",
      "        True       0.63      0.17      0.26      4732\n",
      "\n",
      "    accuracy                           0.56     10000\n",
      "   macro avg       0.59      0.54      0.48     10000\n",
      "weighted avg       0.59      0.56      0.49     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\"QWEN NLI Metrics\")\\nprint(evaluate_nli(qwen_nli_results, df_valid_qwen[\\'groq_qwen_is_hallucinated\\']))'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(\"LLAMA NLI Metrics\")\n",
    "print(evaluate_nli(llama_nli_results, df_valid_llama['together_llama_is_hallucinated']))'''\n",
    "print(\"Gemma NLI Metrics\")\n",
    "gemma_report, gemma_false_negs = evaluate_nli(gemma_nli_results, df_valid_gemma['together_gemma_is_hallucinated'])\n",
    "print(gemma_report)\n",
    "'''print(\"QWEN NLI Metrics\")\n",
    "print(evaluate_nli(qwen_nli_results, df_valid_qwen['groq_qwen_is_hallucinated']))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d04b74-36ce-444d-83f9-0c2fc72a05d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: Cadmium chloride is slightly soluble in water.\n",
      "Label: contradiction (Confidence: 0.99820)\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "def clean_markdown(text):\n",
    "    return re.sub(r'[*_~`]+', '', text)  # removes markdown symbols like **, __, ~~ etc.\n",
    "\n",
    "# Load fact-checking model\n",
    "fact_checker = pipeline(\"text-classification\", model=\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "\n",
    "# Function to verify a claim\n",
    "def verify_claim_with_wikipedia(claim, top_k=1):\n",
    "    try:\n",
    "        # Search Wikipedia\n",
    "        search_results = wikipedia.search(claim)\n",
    "        if not search_results:\n",
    "            return \"NO WIKI RESULTS\", None\n",
    "\n",
    "        # Fetch top result's summary\n",
    "        page = wikipedia.page(search_results[0])\n",
    "        premise = page.summary\n",
    "\n",
    "        # Prepare input for NLI\n",
    "        nli_input = f\"{premise} [SEP] {claim}\"\n",
    "        result = fact_checker(nli_input)[0]\n",
    "\n",
    "        return result['label'], result['score']\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", str(e)\n",
    "\n",
    "# Example claim\n",
    "claim = \"Cadmium chloride is slightly soluble in **water**.\"\n",
    "claim = clean_markdown(claim)\n",
    "\n",
    "label, score = verify_claim_with_wikipedia(claim)\n",
    "print(f\"Claim: {claim}\\nLabel: {label} (Confidence: {score:.5f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff6dc8-3b7e-4a6f-b803-db439a058c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
